{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7022722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import hashlib\n",
    "import requests\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ce3a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "\n",
    "        # Count token frequencies\n",
    "        counter = Counter()\n",
    "        for seq in tokens:\n",
    "            counter.update(seq)\n",
    "\n",
    "        # Sort by frequency\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Add reserved tokens (unk, pad, bos, eos)\n",
    "        self.idx_to_token = [\"<unk>\"] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "        # Add tokens that meet frequency threshold\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7caaca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTFraEng(Dataset):\n",
    "    \"\"\"English-French machine translation dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, batch_size=32, num_steps=10, num_train=512, num_val=128, root=\"./data\"\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.num_train = num_train\n",
    "        self.num_val = num_val\n",
    "        self.root = root\n",
    "\n",
    "        # Download and process data\n",
    "        raw_text = self._download()\n",
    "        text = self._preprocess(raw_text)\n",
    "        src, tgt = self._tokenize(text, num_train + num_val)\n",
    "\n",
    "        # Build arrays and vocabularies\n",
    "        self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(src, tgt)\n",
    "\n",
    "    def _download(self):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        os.makedirs(self.root, exist_ok=True)\n",
    "        url = \"http://d2l-data.s3-accelerate.amazonaws.com/fra-eng.zip\"\n",
    "        zip_path = os.path.join(self.root, \"fra-eng.zip\")\n",
    "\n",
    "        # Download if not exists\n",
    "        if not os.path.exists(zip_path):\n",
    "            print(f\"Downloading {url}...\")\n",
    "            response = requests.get(url)\n",
    "            with open(zip_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "        # Extract\n",
    "        extract_path = os.path.join(self.root, \"fra-eng\")\n",
    "        if not os.path.exists(extract_path):\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(self.root)\n",
    "\n",
    "        # Read file\n",
    "        with open(os.path.join(extract_path, \"fra.txt\"), encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"Preprocess the raw text.\"\"\"\n",
    "        # Replace non-breaking space with space\n",
    "        text = text.replace(\"\\u202f\", \" \").replace(\"\\xa0\", \" \")\n",
    "\n",
    "        # Insert space between words and punctuation marks\n",
    "        def no_space(char, prev_char):\n",
    "            return char in set(\",.!?\") and prev_char != \" \"\n",
    "\n",
    "        out = [\n",
    "            \" \" + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "            for i, char in enumerate(text.lower())\n",
    "        ]\n",
    "        return \"\".join(out)\n",
    "\n",
    "    def _tokenize(self, text, max_examples=None):\n",
    "        \"\"\"Tokenize the text into source and target sequences.\"\"\"\n",
    "        src, tgt = [], []\n",
    "        for i, line in enumerate(text.split(\"\\n\")):\n",
    "            if max_examples and i >= max_examples:\n",
    "                break\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                # Split on spaces and filter empty tokens\n",
    "                src.append([t for t in f\"{parts[0]} <eos>\".split(\" \") if t])\n",
    "                tgt.append([t for t in f\"{parts[1]} <eos>\".split(\" \") if t])\n",
    "        return src, tgt\n",
    "\n",
    "    def _build_arrays(self, src, tgt, src_vocab=None, tgt_vocab=None):\n",
    "        \"\"\"Build arrays from tokenized sequences.\"\"\"\n",
    "\n",
    "        def _build_array(sentences, vocab, is_tgt=False):\n",
    "            # Pad or trim sequences to num_steps\n",
    "            def pad_or_trim(seq, t):\n",
    "                if len(seq) > t:\n",
    "                    return seq[:t]  # Trim\n",
    "                else:\n",
    "                    return seq + [\"<pad>\"] * (t - len(seq))  # Pad\n",
    "\n",
    "            sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n",
    "\n",
    "            # Add <bos> i.e. beginning-of-sequence token for target sequences\n",
    "            if is_tgt:\n",
    "                sentences = [[\"<bos>\"] + s for s in sentences]\n",
    "\n",
    "            # Build vocabulary\n",
    "            if vocab is None:\n",
    "                vocab = Vocab(\n",
    "                    sentences, min_freq=2, reserved_tokens=[\"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "                )\n",
    "\n",
    "            # Convert to indices\n",
    "            array = torch.tensor([vocab[s] for s in sentences])\n",
    "\n",
    "            # Calculate valid lengths (excluding padding)\n",
    "            valid_len = (array != vocab[\"<pad>\"]).type(torch.int32).sum(1)\n",
    "\n",
    "            return array, vocab, valid_len\n",
    "\n",
    "        # Build source arrays\n",
    "        src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\n",
    "\n",
    "        # Build target arrays\n",
    "        tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, is_tgt=True)\n",
    "\n",
    "        # Return: (src, tgt_input, src_valid_len, tgt_label)\n",
    "        # tgt_input: <bos> + tokens (excluding last)\n",
    "        # tgt_label: tokens + <eos> (excluding <bos>)\n",
    "        return (\n",
    "            (src_array, tgt_array[:, :-1], src_valid_len, tgt_array[:, 1:]),\n",
    "            src_vocab,\n",
    "            tgt_vocab,\n",
    "        )\n",
    "\n",
    "    def get_dataloader(self, train=True):\n",
    "        \"\"\"Get dataloader for training or validation.\"\"\"\n",
    "        src_array, tgt_array, src_valid_len, label = self.arrays\n",
    "\n",
    "        if train:\n",
    "            indices = slice(0, self.num_train)\n",
    "        else:\n",
    "            indices = slice(self.num_train, self.num_train + self.num_val)\n",
    "\n",
    "        # Create TensorDataset\n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            src_array[indices],\n",
    "            tgt_array[indices],\n",
    "            src_valid_len[indices],\n",
    "            label[indices],\n",
    "        )\n",
    "\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=train)\n",
    "\n",
    "    def build(self, src_sentences, tgt_sentences):\n",
    "        \"\"\"Build arrays for new sentence pairs using existing vocabularies.\"\"\"\n",
    "        raw_text = \"\\n\".join(\n",
    "            [src + \"\\t\" + tgt for src, tgt in zip(src_sentences, tgt_sentences)]\n",
    "        )\n",
    "        text = self._preprocess(raw_text)\n",
    "        src, tgt = self._tokenize(text)\n",
    "        arrays, _, _ = self._build_arrays(src, tgt, self.src_vocab, self.tgt_vocab)\n",
    "        return arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d369883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.MTFraEng at 0x7881f12de170>, 195, 213)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = MTFraEng(batch_size=3, num_steps=9, num_train=512, num_val=128)\n",
    "\n",
    "data, len(data.src_vocab), len(data.tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6376b8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: tensor([[  5,  96,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  7,   0,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 10, 124,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 38,  12,   6,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  0,  82,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 29,  57,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  7, 100,   6,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 38,  10,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  0,  48,  11,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  9,  73,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  5,  52,  75,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [  7,  58,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  5,   0,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  5,  96,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 15,  27,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 50,  51,   6,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [165,   9,   6,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  7,  75,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  0,   8,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  5,   0,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 62,   6,   3,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [  9,   4,   3,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [ 18,   5, 107,  11,   3,   1,   1,   1,   1,   1],\n",
      "        [  7,   0,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  5,  99,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 17,  22,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 26,  34,   6,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [166,  36,   6,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  5,  74,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  7,  10,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  9,  83,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [135, 107,   4,   3,   1,   1,   1,   1,   1,   1]], dtype=torch.int32)\n",
      "decoder input: tensor([[  2,   6,   7,   0,   4,   3,   1,   1,   1,   1],\n",
      "        [  2,  10,   0,   0,   4,   3,   1,   1,   1,   1],\n",
      "        [  2,   8,  47,  73,   4,   3,   1,   1,   1,   1],\n",
      "        [  2, 136,   5,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  2,   0,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  2, 152,  71,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [  2,   6,   7,   0,   5,   3,   1,   1,   1,   1],\n",
      "        [  2, 160,   8,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [  2,  25,   0,   9,   3,   1,   1,   1,   1,   1],\n",
      "        [  2,  19, 107,  58,   4,   3,   1,   1,   1,   1],\n",
      "        [  2,   6,  49,   7,  18,   0,   4,   3,   1,   1],\n",
      "        [  2,   6,   7,  38,  48,   4,   3,   1,   1,   1],\n",
      "        [  2,  20,   0,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [  2,   6,   7, 113,   4,   3,   1,   1,   1,   1],\n",
      "        [  2,  22,   0,   5,   3,   1,   1,   1,   1,   1],\n",
      "        [  2,  36,  89,   5,   3,   1,   1,   1,   1,   1],\n",
      "        [  2, 178,   5,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  2,  11,  19,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [  2,  62,   5,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  2,  10,   0,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [  2,  11,   0,   5,   3,   1,   1,   1,   1,   1],\n",
      "        [  2,  19,   5,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  2,  86,  21, 169,   9,   3,   1,   1,   1,   1],\n",
      "        [  2,  34,  91,   0,   4,   3,   1,   1,   1,   1],\n",
      "        [  2,  10,   0,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [  2,  13,   0,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [  2,  14, 116,  33,  74,   5,   3,   1,   1,   1],\n",
      "        [  2,   0,   0,   5,   3,   1,   1,   1,   1,   1],\n",
      "        [  2,   6,   7, 114,   4,   3,   1,   1,   1,   1],\n",
      "        [  2,   6,   7,   8,   4,   3,   1,   1,   1,   1],\n",
      "        [  2,  72,   5,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  2,   0, 146,   4,   3,   1,   1,   1,   1,   1]], dtype=torch.int32)\n",
      "source len excluding pad: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 5, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "label: tensor([[  6,   7,   0,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [ 10,   0,   0,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [  8,  47,  73,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [136,   5,   3,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [  0,   4,   3,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [152,  71,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  6,   7,   0,   5,   3,   1,   1,   1,   1,   1],\n",
      "        [160,   8,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 25,   0,   9,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 19, 107,  58,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [  6,  49,   7,  18,   0,   4,   3,   1,   1,   1],\n",
      "        [  6,   7,  38,  48,   4,   3,   1,   1,   1,   1],\n",
      "        [ 20,   0,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  6,   7, 113,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [ 22,   0,   5,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 36,  89,   5,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [178,   5,   3,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [ 11,  19,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 62,   5,   3,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [ 10,   0,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 11,   0,   5,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 19,   5,   3,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [ 86,  21, 169,   9,   3,   1,   1,   1,   1,   1],\n",
      "        [ 34,  91,   0,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [ 10,   0,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 13,   0,   4,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [ 14, 116,  33,  74,   5,   3,   1,   1,   1,   1],\n",
      "        [  0,   0,   5,   3,   1,   1,   1,   1,   1,   1],\n",
      "        [  6,   7, 114,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [  6,   7,   8,   4,   3,   1,   1,   1,   1,   1],\n",
      "        [ 72,   5,   3,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [  0, 146,   4,   3,   1,   1,   1,   1,   1,   1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "train_loader = data.get_dataloader(train=True)\n",
    "src, tgt, src_valid_len, label = next(iter(train_loader))\n",
    "print(\"source:\", src.type(torch.int32))\n",
    "print(\"decoder input:\", tgt.type(torch.int32))\n",
    "print(\"source len excluding pad:\", src_valid_len.type(torch.int32))\n",
    "print(\"label:\", label.type(torch.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21eb10",
   "metadata": {},
   "source": [
    "unlike character-level tokenization, for machine translation we\n",
    "prefer word-level tokenization here (state-of-the-art models use more complex tokenization techniques).\n",
    "\n",
    "\n",
    "For languages that doesn't have clear spaces between words like japanese, we do subword tokenization(BPE, WordPiece, SentencePiece)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae4da8",
   "metadata": {},
   "source": [
    "In machine translation, we have paris like:\n",
    "\n",
    "\"I love cats\" 3 tokens\n",
    "\n",
    "\"J'adore les chats\" 3 tokens\n",
    "\n",
    "but also \n",
    "\n",
    "English: \"How are you doing today?\" (5 tokens)\n",
    "\n",
    "French: \"Comment allez-vous aujourd'hui?\" (3 tokens)\n",
    "\n",
    "but nns work with fixed-size tensors. you can't have a batch where one example has 3 tokens and another has 5.\n",
    "\n",
    "The solution is Padding and Truncation:\n",
    "\n",
    "Let's say we will use `num_steps = 5`:\n",
    "\n",
    "Example 1 (too short):\n",
    "```\n",
    "Original: [\"hi\", \"<eos>\"]  (2 tokens)\n",
    "Padded:   [\"hi\", \"<eos>\", \"<pad>\", \"<pad>\", \"<pad>\"]  (5 tokens)\n",
    "```\n",
    "Example 2 (perfect fit):\n",
    "```\n",
    "Original: [\"how\", \"are\", \"you\", \"?\", \"<eos>\"]  (5 tokens)\n",
    "No change: [\"how\", \"are\", \"you\", \"?\", \"<eos>\"]\n",
    "```\n",
    "Example 3 (too long):\n",
    "```\n",
    "Original: [\"what\", \"are\", \"you\", \"doing\", \"today\", \"?\", \"<eos>\"]  (7 tokens)\n",
    "Truncated: [\"what\", \"are\", \"you\", \"doing\", \"today\"]  (5 tokens)\n",
    "```\n",
    "Now all sequences are exactly 5 tokens → can stack into a batch tensor!\n",
    "\n",
    "\n",
    "We also need to track the valid length to ignore the padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db691a7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d83f12",
   "metadata": {},
   "source": [
    "In general seq2seq problems like machine translation,\n",
    "inputs and outputs are of varying lenghts that are unaligned.\n",
    "The standard approach to handle this sort of data is to design an\n",
    "encoder-decoder architecture,\n",
    "\n",
    "consisting of :\n",
    "\n",
    "1. An encoder that takes a variable lenght sequence as input\n",
    "2. a decoder that acts as a conditional language model, taking in the encoded input and the tleftward context of the target sequence and predicting the subsequent token in the target sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68077b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d4443fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def init_states(enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        # Run encoder\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "\n",
    "        # Initialize decoder state using encoder outputs\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "\n",
    "        # Run decoder and return only its output\n",
    "        dec_outputs, _ = self.decoder(dec_X, dec_state)\n",
    "        return dec_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87249ff5",
   "metadata": {},
   "source": [
    "In this section, we will demonstrate the application of an encoder–decoder architecture, where both the encoder and decoder are implemented as RNNs, to the task of machine translation \n",
    "\n",
    "\n",
    "Here, the encoder RNN will take a variable-length sequence as input and transform it into a fixed-shape hidden state. Later, in Section 11, we will introduce attention mechanisms, which allow us to access encoded inputs without having to compress the entire input into a single fixed-length representation.\n",
    "\n",
    "\n",
    "Then to generate the output sequence, one token at a time, the decoder model, consisting of a separate RNN, will predict each successive target token given both the input sequence and the preceding tokens in the output.\n",
    "\n",
    "\n",
    "## Teacher Forcing\n",
    "In a standard sequence generation process (like translation)\n",
    ", the decoder produces its output one token at a time.\n",
    "The input for the next time step is the token that the decoder predicted at the current time step.\n",
    "\n",
    "However, during training,\n",
    "if the decoder makes a mistake early in the sequence, \n",
    "that mistake is fed back as the input for the next step,\n",
    "potentially compounding errors and confusing the training process. \n",
    "The model might never recover from a single bad prediction, leading to unstable learning.\n",
    "\n",
    "The most common approach is sometimes called *teacher forcing*.\n",
    "\n",
    "Teacher Forcing circumvents this instability by feeding the decoder the correct token from the target sequence at every time step, regardless of what the decoder actually predicted.\n",
    "\n",
    "The \"Teacher\" (the ground truth target sequence) forces the network to follow the correct path, ensuring that the model's recurrent states are always updated based on accurate context.\n",
    "\n",
    "More concretely, the special beginning-of-sequence token and the original target sequence, excluding the final token, \n",
    "are concatenated as input to the decoder, \n",
    "while the decoder output (labels for training) is the original target sequence,\n",
    "shifted by one token: \n",
    "\n",
    "$$\n",
    "\\text{\"<bos>\", \"Ils\", \"regardent\", \".\"}\\Rightarrow\\text{\"Ils\", \"regardent\", \".\", \"<eos>\"}\n",
    "$$\n",
    "\n",
    "![](../imgs/seq2seq.png)\n",
    "\n",
    "another approach is to feed the predicted token from the previous time step as the current inpt.\n",
    "While this is how the model is used in the real world, it is often too difficult to train with, as errors quickly compound. Teacher Forcing provides a smooth, stable gradient path during training, helping the decoder learn the correct local transitions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f2e32",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The Encoder reads the entire source sequence \n",
    "(e.g., an English sentence) and converts it into a single, fixed-length vector known as the **Context Vector** (or latent state).\n",
    "\n",
    "The encoder typically uses a unidirectional or bidirectional RNN \n",
    "(e.g., an LSTM or GRU). \n",
    "At each time step $t$, the RNN processes the input token $x_t$ and updates its hidden state $\\mathbf{h}_t$.\n",
    "\n",
    "For an input sequence $\\mathbf{X} = (x_1, x_2, \\ldots, x_T)$:\n",
    "\n",
    "1. Input: Each token $x_t$ is transformed into a feature vector $\\mathbf{x}_t$ (embedding).\n",
    "\n",
    "2. Recurrence: The hidden state $\\mathbf{h}_t$ is computed based on the current input $\\mathbf{x}_t$ and the previous state $\\mathbf{h}_{t-1}$:$$\\mathbf{h}_t = f(\\mathbf{x}_t, \\mathbf{h}_{t-1})$$\n",
    "\n",
    "3. Context Vector: After processing the final token $x_T$,\n",
    "the encoder's final hidden state, \n",
    "$\\mathbf{h}_T$, serves as the Context Vector. \n",
    "This vector is intended to summarize all the information of the entire source sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb2230b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(Encoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 1. Embedding Layer: Maps tokens to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 2. RNN Layer: Processes the sequence\n",
    "        # (e.g., using GRU or LSTM)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # X: (batch size, sequence length)\n",
    "        X = self.embedding(X)\n",
    "        # X: (sequence length, batch size, embed size)\n",
    "        X = X.permute(1, 0, 2)\n",
    "\n",
    "        # rnn returns output and final hidden state (state)\n",
    "        output, state = self.rnn(X)\n",
    "\n",
    "        # output is usually ignored in the simplest Seq2Seq model\n",
    "        # state is the context vector fed to the decoder\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419a02f",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "The **decoder** takes the context vecotr from the encoder and generates the output sequence one token at a time.\n",
    "It operates **autoregressively**, meaning the prediction at the current step is influenced by the prediction made at the previous step.\n",
    "\n",
    "The decoder is also an RNN.\n",
    "\n",
    "1. Initial State: The decoder's initial hidden state is set directly to the \n",
    "**Context Vector** ($h_T$) produced by the encoder.\n",
    "This is how the information form the source sequence is transferred.\n",
    "\n",
    "2. Input: At each time step $t'$, the decoder takes the token generated at the previous time step $t' -1$.\n",
    "\n",
    "The decoder's recurrence is similar to the encoder's, but it includes an output layer:\n",
    "1. Recurrence: The decoder's hidden state $\\mathbf{s}_{t'}$ is calculated using its input \n",
    "$y_{t'-1}$ and its previous state $\\mathbf{s}_{t'-1}$:\n",
    "$$\\mathbf{s}_{t'} = g(y_{t'-1}, \\mathbf{s}_{t'-1}, \\mathbf{h}_T)$$\n",
    "2. Output: The hidden state $\\mathbf{s}_{t'}$ is used to predict the next token $\\hat{y}_{t'}$ via a fully connected layer and a Softmax function:\n",
    "$$\\hat{y}_{t'} = \\text{softmax}(\\mathbf{s}_{t'} \\mathbf{W}_{qs} + \\mathbf{b}_q)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "199e2893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(Decoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Input size: embed_size + num_hiddens (for context)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers)\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        # enc_outputs is typically a tuple of (all_encoder_outputs, final_hidden_state)\n",
    "        # We take the final hidden state (index 1) to initialize the decoder RNN\n",
    "        return enc_outputs\n",
    "\n",
    "    def forward(self, Y, state):\n",
    "        # Y shape: (batch_size, num_steps)\n",
    "\n",
    "        # 1. Embedding and Transpose: (seq_len, batch_size, embed_size)\n",
    "        embs = self.embedding(Y.type(torch.long)).permute(1, 0, 2)\n",
    "\n",
    "        # 2. Unpack Encoder State\n",
    "        enc_output, hidden_state = (\n",
    "            state  # hidden_state is (num_layers, batch_size, num_hiddens)\n",
    "        )\n",
    "\n",
    "        # 3. CONTEXT EXTRACTION (CRITICAL FIX)\n",
    "        # Use the final hidden state of the last encoder layer as the context vector\n",
    "        # context_vector shape is (batch_size, num_hiddens) -> (4, 16)\n",
    "        context_vector = hidden_state[-1]\n",
    "\n",
    "        # Add a sequence dimension of size 1, then repeat (broadcast)\n",
    "        # Final context shape: (seq_len, batch_size, num_hiddens) -> (9, 4, 16)\n",
    "        context = context_vector.unsqueeze(0).repeat(embs.shape[0], 1, 1)\n",
    "\n",
    "        # 4. Concatenation (seq_len, batch_size, embed_size + num_hiddens)\n",
    "        embs_and_context = torch.cat((embs, context), -1)\n",
    "\n",
    "        # 5. RNN Forward Pass\n",
    "        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n",
    "\n",
    "        # 6. Final Prediction and Transpose back to (batch_size, seq_len, vocab_size)\n",
    "        output = self.dense(outputs).swapaxes(0, 1)\n",
    "\n",
    "        # Return predictions and the updated state\n",
    "        return output, [enc_output, hidden_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57dcc815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_shape(tensor, expected_shape):\n",
    "    \"\"\"\n",
    "    Placeholder for d2l.check_shape, prints success or failure.\n",
    "    \"\"\"\n",
    "    assert tensor.shape == expected_shape, (\n",
    "        f\"Shape mismatch: Got {tensor.shape}, Expected {expected_shape}\"\n",
    "    )\n",
    "    print(f\"Shape check successful for tensor with shape: {expected_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "687a5cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing Encoder Shapes ---\n",
      "Shape check successful for tensor with shape: (9, 4, 16)\n",
      "Shape check successful for tensor with shape: (2, 4, 16)\n",
      "\n",
      "--- Testing Decoder Shapes ---\n",
      "Shape check successful for tensor with shape: (4, 9, 10)\n",
      "Shape check successful for tensor with shape: (2, 4, 16)\n",
      "\n",
      "All shape checks passed successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Parameters ---\n",
    "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
    "batch_size, num_steps = 4, 9\n",
    "\n",
    "# --- Dummy Input Data ---\n",
    "# Use Long tensor for inputs to embedding layer\n",
    "X = torch.zeros((batch_size, num_steps), dtype=torch.long)\n",
    "\n",
    "print(\"--- Testing Encoder Shapes ---\")\n",
    "encoder = Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "\n",
    "# ENCODER FORWARD PASS\n",
    "enc_outputs, enc_state = encoder(X)\n",
    "\n",
    "# 1. Check Encoder Output Sequence (All hidden states)\n",
    "# Shape: (sequence length, batch size, number of hidden units)\n",
    "check_shape(enc_outputs, (num_steps, batch_size, num_hiddens))\n",
    "\n",
    "# 2. Check Encoder Final State (Context Vector stack)\n",
    "# Shape: (number of layers, batch size, number of hidden units)\n",
    "check_shape(enc_state, (num_layers, batch_size, num_hiddens))\n",
    "\n",
    "\n",
    "print(\"\\n--- Testing Decoder Shapes ---\")\n",
    "decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "\n",
    "# Initialize Decoder State: Get the final hidden state from the encoder\n",
    "# NOTE: encoder(X) returns (enc_outputs, enc_state)\n",
    "initial_state = decoder.init_state(encoder(X))\n",
    "\n",
    "# DECODER FORWARD PASS\n",
    "# Decoder input X is the target sequence (Teacher Forcing)\n",
    "dec_outputs, state = decoder(X, initial_state)\n",
    "\n",
    "# 3. Check Decoder Output Predictions\n",
    "# Shape: (batch size, sequence length, vocabulary size)\n",
    "check_shape(dec_outputs, (batch_size, num_steps, vocab_size))\n",
    "\n",
    "# 4. Check Decoder Final State (The recurrent hidden state stack)\n",
    "# state[1] is the (num_layers, batch_size, num_hiddens) state of the decoder RNN\n",
    "check_shape(state[1], (num_layers, batch_size, num_hiddens))\n",
    "\n",
    "print(\"\\nAll shape checks passed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a88adb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    maxlen = X.size(1)\n",
    "    mask = (\n",
    "        torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :]\n",
    "        < valid_len[:, None]\n",
    "    )\n",
    "\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "\n",
    "class MaskedLoss(nn.CrossEntropyLoss):\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction = \"none\"\n",
    "        unweighted_loss = super().forward(pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45bb3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(EncoderDecoder):\n",
    "    def __init__(self, encoder, decoder, tgt_pad, lr):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.tgt_pad = tgt_pad\n",
    "        self.lr = lr\n",
    "        self.loss = MaskedLoss()\n",
    "\n",
    "    def forward(self, enc_X, dec_X, enc_valid_len, *args):\n",
    "        # Passes source X and target X to the base EncoderDecoder\n",
    "        return super().forward(enc_X, dec_X, *args)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        # Batch items: (Source X, Source X_lengths, Target Y, Target Y_lengths)\n",
    "        # Assuming the DataLoader prepares: enc_X, dec_X, enc_valid_len, dec_valid_len\n",
    "        # NOTE: dec_X must be the shifted target sequence (<bos> + Y[:-1])\n",
    "        # The labels Y_labels should be the unshifted target sequence (Y[1:] + <eos>)\n",
    "\n",
    "        enc_X, dec_X, enc_valid_len, Y_labels = (\n",
    "            batch  # Assuming you add Y_labels to batch\n",
    "        )\n",
    "\n",
    "        dec_valid_len = (Y_labels != self.tgt_pad).sum(1)\n",
    "\n",
    "        # 1. Forward Pass: Get predictions\n",
    "        Y_hat = self(enc_X, dec_X, enc_valid_len)\n",
    "\n",
    "        # 2. Compute Loss\n",
    "        # Flatten predictions and labels for loss calculation\n",
    "        loss = self.loss(Y_hat, Y_labels, dec_valid_len).sum()\n",
    "\n",
    "        # In a real D2L/PyTorch Lightning environment, this would log the loss:\n",
    "        # self.plot('loss', loss, train=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        # Performs the same calculation for evaluation\n",
    "        enc_X, dec_X, enc_valid_len, dec_valid_len = batch\n",
    "        Y_hat = self(enc_X, dec_X, enc_valid_len, dec_valid_len)\n",
    "        Y_labels = batch[3]\n",
    "        loss = self.loss(Y_hat, Y_labels, dec_valid_len).sum()\n",
    "\n",
    "        # In a real setup, you'd track metrics like BLEU score here\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, num_steps, device, save_attention_weights=False):\n",
    "        # ... (Move data to device, unpack batch)\n",
    "        batch = [a.to(device) for a in batch]\n",
    "        src, tgt, src_valid_len, _ = batch\n",
    "\n",
    "        # 1. Encoder Forward Pass\n",
    "        enc_all_outputs = self.encoder(src, src_valid_len)\n",
    "\n",
    "        # 2. Decoder State Initialization\n",
    "        dec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)\n",
    "\n",
    "        # 3. Initialize Output Sequence\n",
    "        # outputs starts with the <bos> token.\n",
    "        # tgt[:, (0)] selects the first token of the target batch (which is <bos>)\n",
    "        # .unsqueeze(1) changes shape from (batch_size,) to (batch_size, 1)\n",
    "        outputs, attention_weights = (\n",
    "            [\n",
    "                tgt[:, (0)].unsqueeze(1),\n",
    "            ],\n",
    "            [],\n",
    "        )\n",
    "\n",
    "        # 4. Decoding Loop (Autoregressive Generation)\n",
    "        for _ in range(num_steps):\n",
    "            # Decode one step: Y is the prediction logits for the next token\n",
    "            # outputs[-1] is the *predicted token* from the previous step (Teacher Forcing is off)\n",
    "            Y, dec_state = self.decoder(outputs[-1], dec_state)\n",
    "\n",
    "            # Greedy Decoding: Take the token with the highest probability (argmax)\n",
    "            # Y.argmax(2) gets the index of the predicted token across the vocab dimension (dim 2)\n",
    "            outputs.append(Y.argmax(2))\n",
    "\n",
    "            # Save attention weights (if applicable)\n",
    "            if save_attention_weights:\n",
    "                attention_weights.append(self.decoder.attention_weights)\n",
    "\n",
    "        # Return: Concatenate the predicted tokens (excluding the initial <bos>)\n",
    "        return torch.cat(outputs[1:], 1), attention_weights\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Standard configuration for the Adam optimizer\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c51591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_seq2seq(model, data_iter, lr, num_epochs, device):\n",
    "    \"\"\"Train a sequence-to-sequence model with enhanced tqdm visualization.\"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = model.configure_optimizers()\n",
    "\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    # --- Outer Loop: Epochs ---\n",
    "    # Use ascii=True for wider compatibility if not in a full Jupyter environment\n",
    "    epoch_pbar = tqdm(range(1, num_epochs + 1), desc=\"Epochs\")\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # --- Inner Loop: Batches ---\n",
    "        # Initialize inner tqdm for batches, linked to the outer loop description\n",
    "        batch_pbar = tqdm(\n",
    "            enumerate(data_iter, 1),\n",
    "            total=len(data_iter),\n",
    "            desc=f\"Epoch {epoch}\",\n",
    "            leave=False,\n",
    "        )\n",
    "\n",
    "        for i, batch in batch_pbar:\n",
    "            # Move data to device\n",
    "            batch = [x.to(device) for x in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = model.training_step(batch)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (essential for RNNs)\n",
    "            grad_clip = 1\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track loss and calculate running average\n",
    "            total_loss += loss.item()\n",
    "            running_avg_loss = total_loss / i\n",
    "\n",
    "            # Update the progress bar description with the current running loss\n",
    "            batch_pbar.set_postfix(train_loss=f\"{running_avg_loss:.4f}\")\n",
    "\n",
    "        # --- End of Epoch ---\n",
    "        avg_train_loss = total_loss / len(data_iter)\n",
    "\n",
    "        # Update the outer loop description with the epoch's final loss\n",
    "        epoch_pbar.set_postfix(avg_epoch_loss=f\"{avg_train_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54347aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 30/30 [00:06<00:00,  4.33it/s, avg_epoch_loss=3.9016]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embed_size = 256\n",
    "num_hiddens = 256\n",
    "num_layers = 2\n",
    "lr = 0.005\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "num_steps = 10  # Corresponds to the sequence length used in MTFraEng\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Initialize Data\n",
    "# We must ensure the `tgt_pad` index is correctly captured.\n",
    "data = MTFraEng(batch_size=batch_size, num_steps=num_steps)\n",
    "train_loader = data.get_dataloader(train=True)\n",
    "tgt_pad_idx = data.tgt_vocab[\"<pad>\"]\n",
    "\n",
    "# 2. Initialize Components\n",
    "encoder = Seq2SeqEncoder(\n",
    "    vocab_size=len(data.src_vocab),\n",
    "    embed_size=embed_size,\n",
    "    num_hiddens=num_hiddens,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "\n",
    "decoder = Seq2SeqDecoder(\n",
    "    vocab_size=len(data.tgt_vocab),\n",
    "    embed_size=embed_size,\n",
    "    num_hiddens=num_hiddens,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "\n",
    "# 3. Initialize Model\n",
    "model = Seq2Seq(encoder=encoder, decoder=decoder, tgt_pad=tgt_pad_idx, lr=lr)\n",
    "\n",
    "# 4. Run Training\n",
    "# NOTE: The training will take a few minutes depending on your device/data size.\n",
    "# If using Jupyternotebook/Colab, consider wrapping the training call in a block.\n",
    "train_seq2seq(model, train_loader, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b0dd868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import collections\n",
    "\n",
    "\n",
    "def bleu(pred_seq, label_seq, k):\n",
    "    pred_tokens, label_tokens = pred_seq.split(\" \"), label_seq.split(\" \")\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "\n",
    "    # 1. Brevity Penalty (BP)\n",
    "    # The term: math.exp(min(0, 1 - len_label / len_pred))\n",
    "    # If len_pred > len_label, 1 - ratio is negative, BP < 1.0. If len_pred >= len_label, BP = 1.0.\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "\n",
    "    # 2. Modified N-gram Precision (for n=1 up to k)\n",
    "    for n in range(1, min(k, len_pred) + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "\n",
    "        # Count n-grams in the Reference (label_subs)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[\" \".join(label_tokens[i : i + n])] += 1\n",
    "\n",
    "        # Check n-grams in the Prediction and apply clipping (modified precision)\n",
    "        for i in range(len_pred - n + 1):\n",
    "            ngram = \" \".join(pred_tokens[i : i + n])\n",
    "            if label_subs[ngram] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[ngram] -= 1  # Clip: decrement available count\n",
    "\n",
    "        # Modified precision P_n = num_matches / total_predicted_n_grams\n",
    "        precision = num_matches / (len_pred - n + 1)\n",
    "\n",
    "        # Combine score: score *= P_n^(0.5^n)\n",
    "        score *= math.pow(precision, math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63417d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va !, bleu,1.000\n",
      "i lost . => j'ai perdu ., bleu,1.000\n",
      "he's calm . => je vais <unk> ., bleu,0.000\n",
      "i'm home . => je suis chez moi ., bleu,1.000\n"
     ]
    }
   ],
   "source": [
    "# The input sentences\n",
    "engs = [\"go .\", \"i lost .\", \"he's calm .\", \"i'm home .\"]\n",
    "fras = [\"va !\", \"j'ai perdu .\", \"il est calme .\", \"je suis chez moi .\"]\n",
    "\n",
    "\n",
    "# 1. Build the batch: Converts the English sentences (src) and a dummy target (tgt)\n",
    "#    into tensors using the vocabulary and padding/trimming.\n",
    "#    The prediction only needs the encoder input (src).\n",
    "preds, _ = model.predict_step(data.build(engs, fras), data.num_steps, \"cuda\")\n",
    "\n",
    "# 2. Iterate and Print Results\n",
    "for en, fr, p in zip(engs, fras, preds):\n",
    "    translation = []\n",
    "    predicted_indices = p.tolist()\n",
    "    for token in data.tgt_vocab.to_tokens(predicted_indices):\n",
    "        if token == \"<eos>\":\n",
    "            break  # Stop at the End-of-Sequence token\n",
    "        translation.append(token)\n",
    "\n",
    "    pred_text = \" \".join(translation)\n",
    "\n",
    "    print(f\"{en} => {pred_text}, bleu,{bleu(pred_text, fr, k=2):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e7923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

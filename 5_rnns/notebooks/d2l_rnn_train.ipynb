{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4869a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "import requests\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c814d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preproccess():\n",
    "    url = \"http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt\"\n",
    "    try:\n",
    "        raw_text = requests.get(url).text\n",
    "    except:\n",
    "        print(\"Error downloading text\")\n",
    "        raw_text = \"Placeholder\"\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", raw_text).lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de5a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_char(text):\n",
    "    return list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba39d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved=[\"<unk>\"]):\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [\n",
    "                token for line in tokens for token in line\n",
    "            ]  # unspread the tokens if they came as lines nested\n",
    "\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freq = sorted(\n",
    "            counter.items(), key=lambda x: x[1], reverse=True\n",
    "        )  # count the tokens in reverse\n",
    "\n",
    "        # add all the unique tokens excecluding the ones with freq lower than min_freq\n",
    "        unique_tokens = set(reserved)\n",
    "        for token, freq in self.token_freq:\n",
    "            if freq >= min_freq:\n",
    "                unique_tokens.add(token)\n",
    "\n",
    "        self.idx_to_token = list(sorted(unique_tokens))\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, \"__len__\") and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token_to_idx[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee9e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus_and_vocab(raw_text):\n",
    "    \"\"\"\n",
    "    return the vocabulary and the corpus(the entire text converted to indicies)\n",
    "    \"\"\"\n",
    "    tokens = tokenize_char(raw_text)\n",
    "    vocab = Vocab(tokens)\n",
    "    corpus = torch.tensor(vocab[tokens], dtype=torch.long)\n",
    "    return corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ed4907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, corpus, seq_length):\n",
    "        self.corpus = corpus\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        returns a slice of the corpus from index to the seq_length\n",
    "        \"\"\"\n",
    "        x = self.corpus[index : index + self.seq_length]\n",
    "        y = self.corpus[index + 1 : index + self.seq_length + 1]\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_OneHot(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # use the linear layer that one-hot vectors\n",
    "        self.input_layer = nn.Linear(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        # maps the character to onehotencoding i.e. 0 => [1,0,0,0,0,...]\n",
    "        one_hot = nn.functional.one_hot(x, num_classes=self.vocab_size).float()\n",
    "\n",
    "        output, state = self.rnn(self.input_layer(one_hot), state)\n",
    "        return self.fc(output), state\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Wrap dataloader with tqdm\n",
    "    loop = tqdm(dataloader, desc=\"Training\")\n",
    "\n",
    "    model.train()\n",
    "    for x, y in loop:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initialize hidden state\n",
    "        hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "        # Forward pass\n",
    "        output, hidden = model(x, hidden)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradient\n",
    "        theta = 1.0\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), theta)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0) * x.size(1)\n",
    "        total_tokens += x.size(0) * x.size(1)\n",
    "\n",
    "        # Update tqdm description\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "520066f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, vocab, start_text, num_chars, device, temperature=1.0):\n",
    "    \"\"\"Generate text using the trained model.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Convert start text to indices\n",
    "    tokens = tokenize_char(start_text.lower())\n",
    "    indices = torch.tensor([vocab[tokens]], dtype=torch.long).to(device)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1, device)\n",
    "\n",
    "    result = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Process the start text\n",
    "        for i in range(len(tokens) - 1):\n",
    "            output, hidden = model(indices[:, i : i + 1], hidden)\n",
    "\n",
    "        # Generate new characters\n",
    "        next_input = indices[:, -1:]\n",
    "        for _ in range(num_chars):\n",
    "            output, hidden = model(next_input, hidden)\n",
    "\n",
    "            # Apply temperature and sample\n",
    "            probs = torch.softmax(output[0, -1] / temperature, dim=0)\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            result += vocab.to_tokens(next_idx)\n",
    "            next_input = torch.tensor([[next_idx]], dtype=torch.long).to(device)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c40cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data...\n",
      "Corpus length: 173428\n",
      "Vocabulary size: 28\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:07<00:00, 43.90it/s, loss=1.52]\n",
      "Training: 100%|██████████| 338/338 [00:07<00:00, 44.61it/s, loss=1.27]\n",
      "Training: 100%|██████████| 338/338 [00:08<00:00, 40.07it/s, loss=1.13]\n",
      "Training: 100%|██████████| 338/338 [00:07<00:00, 45.73it/s, loss=1.04]\n",
      "Training: 100%|██████████| 338/338 [00:07<00:00, 44.64it/s, loss=0.962]\n",
      "Epochs:  50%|█████     | 5/10 [00:38<00:38,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.9957, Perplexity: 2.71\n",
      "Sample: the time traveller disk vanishing the lamp and i shall be hand found that a least we came to a stood looking at himself\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 338/338 [00:07<00:00, 44.84it/s, loss=0.886]\n",
      "Training: 100%|██████████| 338/338 [00:08<00:00, 41.16it/s, loss=0.836]\n",
      "Training: 100%|██████████| 338/338 [00:07<00:00, 44.04it/s, loss=0.792]\n",
      "Training: 100%|██████████| 338/338 [00:07<00:00, 44.72it/s, loss=0.761]\n",
      "Training: 100%|██████████| 338/338 [00:07<00:00, 45.06it/s, loss=0.716]\n",
      "Epochs: 100%|██████████| 10/10 [01:17<00:00,  7.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.7413, Perplexity: 2.10\n",
      "Sample: the time traveller and to the bronze penerally adapted the editor econom covered from for an overcame me it by my ratis\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Final Generated Texts:\n",
      "================================================================================\n",
      "\n",
      "Prompt: 'the time traveller '\n",
      "Generated: the time traveller and there and cut off was blown our revivenient pa\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt: 'it was '\n",
      "Generated: it was hiddled me again to my own interminute seeliking a\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt: 'the machine '\n",
      "Generated: the machine lit herments return away of looking at time and pa\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "seq_length = 35  # How many chars it looks at before predicting the next char\n",
    "batch_size = 512\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading data...\")\n",
    "text = load_and_preproccess()\n",
    "corpus, vocab = build_corpus_and_vocab(text)\n",
    "\n",
    "print(f\"Corpus length: {len(corpus)}\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(corpus, seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Initialize model\n",
    "model = RNN_OneHot(len(vocab), hidden_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "    avg_loss = train_epoch(model, dataloader, criterion, optimizer, device)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\"\n",
    "        )\n",
    "\n",
    "        # Generate sample text\n",
    "        sample = predict(\n",
    "            model, vocab, \"the time traveller \", 100, device, temperature=0.8\n",
    "        )\n",
    "        print(f\"Sample: {sample}\\n\")\n",
    "\n",
    "# Final text generation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Final Generated Texts:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "prompts = [\"the time traveller \", \"it was \", \"the machine \"]\n",
    "for prompt in prompts:\n",
    "    generated = predict(\n",
    "        model, vocab, prompt, num_chars=50, device=device, temperature=0.8\n",
    "    )\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
